{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.cache/pypoetry/virtualenvs/swiggle-H3gdtMpT-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from models.sae import SAE\n",
    "from vae_interp.dataset import NpyDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from vae_interp.analysis import get_similar_features, get_activations_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SAE()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config_path = \"./checkpoints/sae/sae_config.json\"\n",
    "# weights_path = \"./checkpoints/sae/sae.pth\"\n",
    "# sae = SAE.load_from_checkpoint(\"../weights/sae/config.json\",\n",
    "#                                \"../weights/sae/sae.pth\")\n",
    "sae = SAE.load_from_checkpoint(\"../checkpoints/sam/lr=1.0e-02_l1=1.0e-02/config.json\",\n",
    "                               \"../checkpoints/sam/lr=1.0e-02_l1=1.0e-02/sae.pth\")\n",
    "sae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NpyDataset(\"../data/vae_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 34/195 [00:00<00:00, 164.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:04<00:00, 45.39it/s]\n"
     ]
    }
   ],
   "source": [
    "activation_info = get_activations_info(sae, dataset, batch_size=256, top_k=1, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1509"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_info.dead_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 9,\n",
       " 11,\n",
       " 12,\n",
       " 14,\n",
       " 15,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 21,\n",
       " 22,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 85,\n",
       " 87,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 100,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 111,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 125,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 133,\n",
       " 136,\n",
       " 137,\n",
       " 139,\n",
       " 140,\n",
       " 142,\n",
       " 144,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 160,\n",
       " 162,\n",
       " 163,\n",
       " 165,\n",
       " 167,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 173,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 200,\n",
       " 201,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 235,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 247,\n",
       " 249,\n",
       " 250,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 262,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 273,\n",
       " 274,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 284,\n",
       " 286,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 293,\n",
       " 294,\n",
       " 297,\n",
       " 299,\n",
       " 301,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 308,\n",
       " 312,\n",
       " 313,\n",
       " 315,\n",
       " 316,\n",
       " 318,\n",
       " 319,\n",
       " 321,\n",
       " 322,\n",
       " 325,\n",
       " 326,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 338,\n",
       " 340,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 353,\n",
       " 354,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 368,\n",
       " 369,\n",
       " 371,\n",
       " 372,\n",
       " 376,\n",
       " 377,\n",
       " 379,\n",
       " 380,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 387,\n",
       " 388,\n",
       " 390,\n",
       " 391,\n",
       " 393,\n",
       " 394,\n",
       " 397,\n",
       " 399,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 411,\n",
       " 413,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 420,\n",
       " 422,\n",
       " 423,\n",
       " 425,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 436,\n",
       " 438,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 449,\n",
       " 450,\n",
       " 452,\n",
       " 453,\n",
       " 455,\n",
       " 456,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 510,\n",
       " 512,\n",
       " 514,\n",
       " 516,\n",
       " 518,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 551,\n",
       " 553,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 584,\n",
       " 585,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 592,\n",
       " 593,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 612,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 631,\n",
       " 632,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 640,\n",
       " 641,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 653,\n",
       " 655,\n",
       " 656,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 683,\n",
       " 684,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 690,\n",
       " 691,\n",
       " 692,\n",
       " 693,\n",
       " 694,\n",
       " 695,\n",
       " 696,\n",
       " 697,\n",
       " 698,\n",
       " 699,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 704,\n",
       " 706,\n",
       " 707,\n",
       " 709,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 713,\n",
       " 715,\n",
       " 716,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 720,\n",
       " 723,\n",
       " 724,\n",
       " 725,\n",
       " 726,\n",
       " 727,\n",
       " 728,\n",
       " 730,\n",
       " 731,\n",
       " 733,\n",
       " 737,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 744,\n",
       " 745,\n",
       " 746,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 756,\n",
       " 757,\n",
       " 758,\n",
       " 761,\n",
       " 762,\n",
       " 764,\n",
       " 765,\n",
       " 767,\n",
       " 768,\n",
       " 771,\n",
       " 774,\n",
       " 775,\n",
       " 776,\n",
       " 778,\n",
       " 779,\n",
       " 780,\n",
       " 782,\n",
       " 783,\n",
       " 784,\n",
       " 785,\n",
       " 787,\n",
       " 788,\n",
       " 790,\n",
       " 791,\n",
       " 792,\n",
       " 793,\n",
       " 794,\n",
       " 795,\n",
       " 796,\n",
       " 797,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 801,\n",
       " 803,\n",
       " 805,\n",
       " 806,\n",
       " 808,\n",
       " 809,\n",
       " 810,\n",
       " 811,\n",
       " 813,\n",
       " 814,\n",
       " 815,\n",
       " 816,\n",
       " 818,\n",
       " 819,\n",
       " 822,\n",
       " 823,\n",
       " 824,\n",
       " 825,\n",
       " 826,\n",
       " 827,\n",
       " 829,\n",
       " 830,\n",
       " 831,\n",
       " 833,\n",
       " 834,\n",
       " 836,\n",
       " 837,\n",
       " 839,\n",
       " 841,\n",
       " 842,\n",
       " 843,\n",
       " 845,\n",
       " 846,\n",
       " 847,\n",
       " 848,\n",
       " 850,\n",
       " 852,\n",
       " 853,\n",
       " 854,\n",
       " 855,\n",
       " 856,\n",
       " 857,\n",
       " 858,\n",
       " 859,\n",
       " 862,\n",
       " 863,\n",
       " 864,\n",
       " 867,\n",
       " 868,\n",
       " 869,\n",
       " 871,\n",
       " 872,\n",
       " 873,\n",
       " 875,\n",
       " 876,\n",
       " 878,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 882,\n",
       " 883,\n",
       " 884,\n",
       " 885,\n",
       " 886,\n",
       " 888,\n",
       " 889,\n",
       " 890,\n",
       " 891,\n",
       " 893,\n",
       " 894,\n",
       " 896,\n",
       " 897,\n",
       " 899,\n",
       " 900,\n",
       " 903,\n",
       " 905,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 909,\n",
       " 911,\n",
       " 912,\n",
       " 913,\n",
       " 914,\n",
       " 915,\n",
       " 916,\n",
       " 917,\n",
       " 919,\n",
       " 920,\n",
       " 921,\n",
       " 923,\n",
       " 924,\n",
       " 925,\n",
       " 927,\n",
       " 928,\n",
       " 929,\n",
       " 930,\n",
       " 931,\n",
       " 932,\n",
       " 934,\n",
       " 935,\n",
       " 938,\n",
       " 939,\n",
       " 940,\n",
       " 941,\n",
       " 942,\n",
       " 944,\n",
       " 945,\n",
       " 946,\n",
       " 947,\n",
       " 948,\n",
       " 949,\n",
       " 950,\n",
       " 952,\n",
       " 953,\n",
       " 956,\n",
       " 958,\n",
       " 959,\n",
       " 960,\n",
       " 962,\n",
       " 963,\n",
       " 965,\n",
       " 966,\n",
       " 968,\n",
       " 969,\n",
       " 970,\n",
       " 972,\n",
       " 974,\n",
       " 976,\n",
       " 977,\n",
       " 978,\n",
       " 981,\n",
       " 982,\n",
       " 984,\n",
       " 985,\n",
       " 986,\n",
       " 988,\n",
       " 989,\n",
       " 990,\n",
       " 991,\n",
       " 992,\n",
       " 994,\n",
       " 996,\n",
       " 997,\n",
       " 998,\n",
       " 999,\n",
       " 1000,\n",
       " 1001,\n",
       " 1002,\n",
       " 1003,\n",
       " 1004,\n",
       " 1005,\n",
       " 1006,\n",
       " 1007,\n",
       " 1008,\n",
       " 1009,\n",
       " 1010,\n",
       " 1012,\n",
       " 1013,\n",
       " 1015,\n",
       " 1016,\n",
       " 1017,\n",
       " 1018,\n",
       " 1019,\n",
       " 1023,\n",
       " 1026,\n",
       " 1028,\n",
       " 1029,\n",
       " 1030,\n",
       " 1031,\n",
       " 1033,\n",
       " 1034,\n",
       " 1035,\n",
       " 1038,\n",
       " 1040,\n",
       " 1041,\n",
       " 1042,\n",
       " 1043,\n",
       " 1044,\n",
       " 1045,\n",
       " 1046,\n",
       " 1047,\n",
       " 1049,\n",
       " 1050,\n",
       " 1051,\n",
       " 1052,\n",
       " 1053,\n",
       " 1054,\n",
       " 1056,\n",
       " 1058,\n",
       " 1059,\n",
       " 1060,\n",
       " 1061,\n",
       " 1062,\n",
       " 1063,\n",
       " 1064,\n",
       " 1066,\n",
       " 1068,\n",
       " 1070,\n",
       " 1072,\n",
       " 1073,\n",
       " 1074,\n",
       " 1075,\n",
       " 1076,\n",
       " 1078,\n",
       " 1079,\n",
       " 1081,\n",
       " 1082,\n",
       " 1084,\n",
       " 1085,\n",
       " 1086,\n",
       " 1087,\n",
       " 1088,\n",
       " 1089,\n",
       " 1090,\n",
       " 1091,\n",
       " 1093,\n",
       " 1095,\n",
       " 1096,\n",
       " 1097,\n",
       " 1098,\n",
       " 1100,\n",
       " 1101,\n",
       " 1102,\n",
       " 1103,\n",
       " 1104,\n",
       " 1105,\n",
       " 1106,\n",
       " 1108,\n",
       " 1109,\n",
       " 1110,\n",
       " 1111,\n",
       " 1112,\n",
       " 1113,\n",
       " 1114,\n",
       " 1115,\n",
       " 1116,\n",
       " 1118,\n",
       " 1121,\n",
       " 1122,\n",
       " 1123,\n",
       " 1124,\n",
       " 1125,\n",
       " 1128,\n",
       " 1129,\n",
       " 1130,\n",
       " 1132,\n",
       " 1133,\n",
       " 1135,\n",
       " 1137,\n",
       " 1139,\n",
       " 1140,\n",
       " 1141,\n",
       " 1142,\n",
       " 1145,\n",
       " 1146,\n",
       " 1147,\n",
       " 1148,\n",
       " 1149,\n",
       " 1150,\n",
       " 1151,\n",
       " 1152,\n",
       " 1155,\n",
       " 1156,\n",
       " 1157,\n",
       " 1158,\n",
       " 1159,\n",
       " 1160,\n",
       " 1161,\n",
       " 1163,\n",
       " 1166,\n",
       " 1167,\n",
       " 1168,\n",
       " 1169,\n",
       " 1170,\n",
       " 1171,\n",
       " 1172,\n",
       " 1173,\n",
       " 1175,\n",
       " 1177,\n",
       " 1178,\n",
       " 1180,\n",
       " 1182,\n",
       " 1183,\n",
       " 1184,\n",
       " 1185,\n",
       " 1187,\n",
       " 1188,\n",
       " 1189,\n",
       " 1190,\n",
       " 1191,\n",
       " 1192,\n",
       " 1193,\n",
       " 1194,\n",
       " 1195,\n",
       " 1196,\n",
       " 1198,\n",
       " 1199,\n",
       " 1200,\n",
       " 1202,\n",
       " 1205,\n",
       " 1206,\n",
       " 1209,\n",
       " 1210,\n",
       " 1212,\n",
       " 1218,\n",
       " 1219,\n",
       " 1220,\n",
       " 1221,\n",
       " 1222,\n",
       " 1223,\n",
       " 1225,\n",
       " 1226,\n",
       " 1227,\n",
       " 1228,\n",
       " 1229,\n",
       " 1230,\n",
       " 1233,\n",
       " 1235,\n",
       " 1236,\n",
       " 1237,\n",
       " 1239,\n",
       " 1240,\n",
       " 1241,\n",
       " 1243,\n",
       " 1246,\n",
       " 1247,\n",
       " 1248,\n",
       " 1250,\n",
       " 1251,\n",
       " 1253,\n",
       " 1254,\n",
       " 1256,\n",
       " 1257,\n",
       " 1258,\n",
       " 1259,\n",
       " 1260,\n",
       " 1263,\n",
       " 1266,\n",
       " 1269,\n",
       " 1270,\n",
       " 1271,\n",
       " 1272,\n",
       " 1273,\n",
       " 1274,\n",
       " 1276,\n",
       " 1278,\n",
       " 1281,\n",
       " 1285,\n",
       " 1286,\n",
       " 1288,\n",
       " 1290,\n",
       " 1291,\n",
       " 1292,\n",
       " 1293,\n",
       " 1294,\n",
       " 1296,\n",
       " 1297,\n",
       " 1298,\n",
       " 1299,\n",
       " 1300,\n",
       " 1301,\n",
       " 1303,\n",
       " 1305,\n",
       " 1306,\n",
       " 1307,\n",
       " 1310,\n",
       " 1311,\n",
       " 1312,\n",
       " 1313,\n",
       " 1314,\n",
       " 1317,\n",
       " 1318,\n",
       " 1320,\n",
       " 1321,\n",
       " 1324,\n",
       " 1326,\n",
       " 1327,\n",
       " 1328,\n",
       " 1329,\n",
       " 1330,\n",
       " 1331,\n",
       " 1332,\n",
       " 1333,\n",
       " 1334,\n",
       " 1335,\n",
       " 1337,\n",
       " 1338,\n",
       " 1339,\n",
       " 1340,\n",
       " 1342,\n",
       " 1343,\n",
       " 1344,\n",
       " 1345,\n",
       " 1346,\n",
       " 1347,\n",
       " 1348,\n",
       " 1349,\n",
       " 1350,\n",
       " 1352,\n",
       " 1354,\n",
       " 1356,\n",
       " 1357,\n",
       " 1358,\n",
       " 1359,\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dead = torch.where(activation_info.activation_density_per_feature == 0)[0].tolist()\n",
    "dead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../dead_neurons.json\", \"w\") as f:\n",
    "    json.dump(dead, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimilarFeatures(k=10, top_k_indices_per_feature=tensor([[287, 415,  76,  ..., 286, 372, 351],\n",
       "        [ 88, 297, 279,  ...,  38, 503, 293],\n",
       "        [130, 489,  36,  ..., 177, 464, 394],\n",
       "        ...,\n",
       "        [363, 423, 136,  ..., 168, 267, 281],\n",
       "        [326,  30, 182,  ..., 244, 193, 386],\n",
       "        [319, 354, 234,  ..., 475, 289, 128]]), top_k_cosine_sim_per_feature=tensor([[0.3491, 0.2737, 0.2525,  ..., 0.2058, 0.2057, 0.2028],\n",
       "        [0.3528, 0.3357, 0.3184,  ..., 0.2759, 0.2707, 0.2610],\n",
       "        [0.2859, 0.2828, 0.2743,  ..., 0.2353, 0.2328, 0.2305],\n",
       "        ...,\n",
       "        [0.4083, 0.3586, 0.3488,  ..., 0.2819, 0.2682, 0.2635],\n",
       "        [0.4350, 0.4086, 0.3748,  ..., 0.3532, 0.3213, 0.3033],\n",
       "        [0.4316, 0.3546, 0.3500,  ..., 0.2843, 0.2796, 0.2766]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similar_features(sae, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 64])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find topk similar features for each feature\n",
    "k = 10\n",
    "num_features = len(sae.features)\n",
    "features_norm = sae.features / torch.linalg.norm(sae.features, dim=1, keepdim=True)\n",
    "features_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 1]), torch.Size([512, 11]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity = features_norm @ features_norm.T\n",
    "topk_indices = torch.topk(cosine_similarity, k=k + 1, dim=1).indices\n",
    "topk_cosine_sim = torch.topk(cosine_similarity, k=k + 1, dim=1).values\n",
    "feature_indices = torch.arange(0, num_features).view(-1, 1)\n",
    "feature_indices.shape, topk_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[287, 415,  76,  ..., 286, 372, 351],\n",
       "        [ 88, 297, 279,  ...,  38, 503, 293],\n",
       "        [130, 489,  36,  ..., 177, 464, 394],\n",
       "        ...,\n",
       "        [363, 423, 136,  ..., 168, 267, 281],\n",
       "        [326,  30, 182,  ..., 244, 193, 386],\n",
       "        [319, 354, 234,  ..., 475, 289, 128]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk = topk_indices[topk_indices != feature_indices].view(num_features, k)\n",
    "topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3491, 0.2737, 0.2525,  ..., 0.2058, 0.2057, 0.2028],\n",
       "        [0.3528, 0.3357, 0.3184,  ..., 0.2759, 0.2707, 0.2610],\n",
       "        [0.2859, 0.2828, 0.2743,  ..., 0.2353, 0.2328, 0.2305],\n",
       "        ...,\n",
       "        [0.4083, 0.3586, 0.3488,  ..., 0.2819, 0.2682, 0.2635],\n",
       "        [0.4350, 0.4086, 0.3748,  ..., 0.3532, 0.3213, 0.3033],\n",
       "        [0.4316, 0.3546, 0.3500,  ..., 0.2843, 0.2796, 0.2766]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_cosine_sim[topk_indices != feature_indices].view(num_features, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/780 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 780/780 [00:05<00:00, 136.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([49859, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find topk activations\n",
    "batch_size = 64\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "sparse_embeddings = None\n",
    "\n",
    "for batch in tqdm(dataloader):\n",
    "    batch_sparse_embeddings = sae.encode(batch)\n",
    "    if sparse_embeddings is None:\n",
    "        sparse_embeddings = batch_sparse_embeddings\n",
    "    else:\n",
    "        sparse_embeddings = torch.cat([sparse_embeddings, batch_sparse_embeddings], dim=0)\n",
    "\n",
    "sparse_embeddings.shape # column is activation density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 34/195 [00:00<00:00, 334.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:01<00:00, 180.45it/s]\n"
     ]
    }
   ],
   "source": [
    "activations_info = get_activations_info(sae, dataset, batch_size=256, top_k=10, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8164, 3.8927, 0.3528, 3.5169, 0.1126, 0.2878, 2.5195, 3.6785, 4.3074,\n",
       "        5.6759, 0.6988, 0.0000, 0.0000, 3.3058, 0.0000, 3.5640, 0.0000, 3.3782,\n",
       "        4.1229, 3.5375, 0.5085, 3.9410, 3.8771, 0.0000, 0.0000, 3.6403, 3.5874,\n",
       "        3.3486, 3.6475, 3.9769, 5.4837, 0.0000, 4.0419, 3.6554, 4.4705, 0.0000,\n",
       "        0.0000, 0.0000, 0.6897, 1.0008, 2.8607, 4.0323, 0.6343, 4.2257, 0.9271,\n",
       "        0.0000, 3.1493, 4.1227, 4.0980, 0.0000, 3.4585, 1.0210, 4.5268, 0.0000,\n",
       "        0.0000, 0.0000, 3.4571, 4.3208, 2.5995, 3.6250, 3.7554, 4.7817, 0.0000,\n",
       "        5.0083, 0.0000, 4.3049, 0.0000, 0.0000, 2.8333, 3.2704, 0.6040, 0.0000,\n",
       "        0.0000, 3.7971, 4.0668, 4.0322, 0.0000, 3.5896, 5.4781, 0.0000, 4.2766,\n",
       "        6.1268, 0.2186, 3.8158, 0.4553, 0.4632, 0.0000, 4.8671, 1.3258, 0.0000,\n",
       "        3.5555, 1.9127, 0.8776, 4.6681, 2.2922, 4.7673, 0.4356, 0.0000, 0.0000,\n",
       "        3.0070, 1.4221, 3.3932, 3.3355, 0.0000, 4.5514, 0.0000, 3.1328, 5.1226,\n",
       "        0.0000, 0.0000, 4.6807, 5.0369, 0.0000, 2.9101, 2.0330, 3.7786, 3.6828,\n",
       "        3.1581, 5.5843, 0.0000, 0.0000, 2.6027, 6.2688, 0.5282, 3.7109, 0.0000,\n",
       "        3.8341, 0.0000, 0.0000, 5.1585, 2.9607, 2.9651, 4.9431, 2.4148, 3.9079,\n",
       "        3.3937, 1.9080, 4.1330, 2.9206, 1.9581, 3.4547, 3.3132, 4.7917, 3.2531,\n",
       "        0.3749, 4.9428, 3.1411, 0.0000, 0.5592, 0.0000, 3.8713, 4.4959, 3.9161,\n",
       "        0.4458, 4.1697, 2.3959, 2.6163, 3.8282, 3.8073, 4.7490, 3.9356, 0.0000,\n",
       "        3.9010, 3.0037, 1.2262, 4.6002, 2.3797, 0.7183, 4.0726, 6.2687, 3.1057,\n",
       "        3.1825, 0.0000, 3.3007, 3.5167, 0.0000, 6.9981, 3.0353, 0.7311, 3.4024,\n",
       "        0.0000, 3.7763, 4.8928, 0.0000, 5.5865, 4.3405, 0.3805, 0.0000, 0.5607,\n",
       "        3.8715, 4.1704, 3.7468, 4.4363, 1.4459, 3.3890, 0.2779, 2.8803, 3.5828,\n",
       "        4.5892, 2.1626, 4.9271, 0.4812, 2.8643, 5.1047, 1.6398, 5.9981, 4.2792,\n",
       "        2.8651, 0.0000, 4.3377, 2.9281, 0.4342, 4.3041, 5.5432, 4.2182, 1.4717,\n",
       "        3.9671, 0.6466, 3.4986, 0.2288, 4.2386, 3.1573, 2.0770, 4.9424, 5.2883,\n",
       "        2.1077, 5.4190, 1.5996, 2.3899, 3.3741, 3.2126, 4.3729, 0.0000, 3.3418,\n",
       "        0.0000, 1.6158, 5.6872, 3.9869, 4.0205, 3.9136, 4.1286, 0.0000, 2.7473,\n",
       "        3.4211, 0.0000, 0.0000, 4.6070, 4.9502, 0.0000, 0.0000, 4.5245, 2.5427,\n",
       "        0.3746, 0.0000, 6.7270, 2.0573, 3.0933, 4.6999, 5.6079, 2.1996, 3.6626,\n",
       "        3.6227, 5.2723, 1.3956, 0.0000, 0.0000, 0.0000, 3.0861, 2.5897, 0.0000,\n",
       "        0.0000, 4.6359, 2.9920, 0.6198, 2.0942, 0.0000, 4.4127, 3.6903, 2.7386,\n",
       "        2.8412, 3.9372, 2.6188, 3.0164, 2.4187, 3.7740, 0.0000, 0.0000, 0.0000,\n",
       "        4.5171, 0.0000, 4.9320, 3.7873, 4.5327, 0.0000, 2.2676, 3.9489, 1.6152,\n",
       "        0.4994, 0.0000, 3.4858, 3.9901, 4.8875, 4.3556, 1.7612, 3.6846, 3.8557,\n",
       "        4.6823, 3.3831, 0.0000, 3.6865, 1.5844, 0.2636, 3.2492, 4.1564, 1.3666,\n",
       "        2.3410, 3.9506, 2.4801, 3.5396, 3.8285, 4.1027, 2.9096, 0.0000, 0.2621,\n",
       "        0.6410, 3.8053, 4.6600, 2.6544, 4.5982, 0.6538, 0.4998, 0.0000, 3.5788,\n",
       "        3.1537, 2.5512, 5.2763, 4.4770, 2.7973, 0.3216, 3.8009, 2.8179, 3.1023,\n",
       "        0.0000, 0.0000, 3.8227, 0.5080, 4.0921, 4.1500, 0.0000, 3.5062, 0.0000,\n",
       "        0.3997, 3.5696, 4.9777, 0.0000, 0.9851, 4.3835, 2.6129, 4.3590, 4.8084,\n",
       "        0.6972, 0.4527, 0.0000, 0.0000, 1.2499, 3.3381, 3.1645, 0.0000, 4.0289,\n",
       "        4.9248, 0.0000, 0.0000, 0.0000, 4.5515, 3.7067, 0.0000, 4.8012, 1.8902,\n",
       "        4.5541, 0.0000, 5.1110, 0.3655, 3.8980, 0.0000, 0.6690, 0.0000, 0.0000,\n",
       "        4.0448, 0.5648, 3.7562, 3.0993, 0.0000, 0.0000, 0.0000, 0.0000, 3.5961,\n",
       "        0.6092, 4.1158, 0.0000, 1.3953, 1.7167, 3.9709, 0.0000, 3.3458, 5.2624,\n",
       "        2.8123, 2.2649, 1.0945, 1.1987, 3.0642, 0.4828, 0.0000, 0.0000, 3.4348,\n",
       "        2.5394, 2.7426, 0.0000, 4.6422, 5.4653, 2.9965, 3.3394, 3.4003, 3.7797,\n",
       "        5.9074, 3.9036, 0.0000, 3.2925, 3.4441, 5.3175, 3.4395, 0.0000, 0.4313,\n",
       "        3.7321, 0.0000, 0.0000, 2.9405, 5.5740, 4.4946, 0.0000, 0.6974, 3.7154,\n",
       "        1.6162, 3.1125, 0.0000, 3.8028, 3.4479, 0.0000, 3.5692, 0.0000, 0.0000,\n",
       "        2.2966, 2.9265, 3.1893, 1.4854, 2.4885, 2.8916, 0.5172, 3.3428, 3.5386,\n",
       "        0.0000, 4.8389, 3.5671, 4.2636, 6.1651, 4.6651, 4.7102, 0.6828, 4.0150,\n",
       "        0.2343, 0.0000, 4.1636, 3.8423, 4.1750, 0.8138, 3.5864, 3.3577, 0.0000,\n",
       "        0.0000, 5.1900, 4.7219, 4.3460, 3.1613, 3.2109, 4.8334, 4.9845, 3.1164,\n",
       "        2.1603, 2.8545, 2.7376, 3.9755, 4.2756, 3.4785, 3.4943, 0.3525, 3.0677,\n",
       "        0.0000, 0.0000, 3.6985, 3.1720, 1.1513, 0.6364, 0.0000, 3.7623, 0.0000,\n",
       "        4.4232, 4.8771, 0.0000, 4.8156, 1.6378, 0.7488, 3.3620, 3.7738])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations_info.max_activation_per_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA!!!\n"
     ]
    }
   ],
   "source": [
    "get_features_pca(sae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rows = sparse embedding of an image feature, cols i = activations for feature i\n",
    "\n",
    "topk_image_activations = torch.topk(sparse_embeddings, k=k, dim=0).indices\n",
    "topk_image_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0772, 0.0000, 0.0511,  ..., 0.0491, 0.0641, 0.0000],\n",
       "        [0.0654, 0.0000, 0.0497,  ..., 0.0474, 0.0611, 0.0000],\n",
       "        [0.0621, 0.0000, 0.0486,  ..., 0.0456, 0.0608, 0.0000],\n",
       "        ...,\n",
       "        [0.0582, 0.0000, 0.0462,  ..., 0.0426, 0.0601, 0.0000],\n",
       "        [0.0579, 0.0000, 0.0460,  ..., 0.0424, 0.0601, 0.0000],\n",
       "        [0.0575, 0.0000, 0.0454,  ..., 0.0423, 0.0600, 0.0000]],\n",
       "       grad_fn=<TopkBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_image_activations = torch.topk(sparse_embeddings, k=k, dim=0).values\n",
    "topk_image_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.8126e-01, 0.0000e+00, 5.1279e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 4.9030e-01, 4.5017e-01, 4.8102e-01, 4.0113e-03, 4.4544e-01,\n",
       "        5.5358e-01, 0.0000e+00, 0.0000e+00, 4.9247e-01, 5.0137e-01, 4.1317e-03,\n",
       "        2.7173e-01, 0.0000e+00, 0.0000e+00, 4.0715e-03, 4.8870e-01, 4.7602e-01,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 5.4634e-02, 1.8994e-02, 0.0000e+00,\n",
       "        8.1430e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.3422e-01,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 4.8336e-03, 4.3902e-01, 4.6307e-01,\n",
       "        8.4639e-03, 3.7506e-03, 0.0000e+00, 4.9407e-01, 3.9311e-03, 3.1942e-01,\n",
       "        4.7656e-01, 2.0337e-02, 3.3051e-01, 0.0000e+00, 8.9653e-03, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 4.3382e-02, 0.0000e+00, 0.0000e+00, 8.5642e-03,\n",
       "        0.0000e+00, 4.4124e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.3836e-03,\n",
       "        0.0000e+00, 4.9688e-01, 6.0791e-02, 3.9913e-03, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5842e-03, 0.0000e+00, 5.2356e-01,\n",
       "        4.9068e-01, 0.0000e+00, 1.2997e-02, 0.0000e+00, 8.8249e-03, 0.0000e+00,\n",
       "        2.0472e-01, 0.0000e+00, 4.3250e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 4.9786e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.9925e-01,\n",
       "        3.3320e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7518e-02,\n",
       "        2.8280e-02, 0.0000e+00, 5.1265e-01, 0.0000e+00, 0.0000e+00, 1.3448e-01,\n",
       "        0.0000e+00, 0.0000e+00, 7.6115e-02, 4.0915e-03, 0.0000e+00, 3.5099e-03,\n",
       "        0.0000e+00, 3.9110e-03, 3.7305e-03, 5.1126e-01, 3.7706e-03, 8.7687e-02,\n",
       "        5.0727e-01, 3.7606e-02, 5.0334e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        1.3919e-02, 0.0000e+00, 5.2215e-01, 0.0000e+00, 0.0000e+00, 5.0430e-01,\n",
       "        0.0000e+00, 4.3924e-03, 0.0000e+00, 4.7534e-03, 0.0000e+00, 0.0000e+00,\n",
       "        3.8709e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.8641e-02,\n",
       "        0.0000e+00, 8.8851e-03, 8.2031e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 4.1718e-03, 0.0000e+00, 3.8509e-03, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 4.9375e-01, 4.9225e-01, 0.0000e+00,\n",
       "        0.0000e+00, 5.1918e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.0118e-02,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 7.1281e-02, 7.7619e-03, 5.4534e-02, 0.0000e+00, 4.3077e-01,\n",
       "        0.0000e+00, 0.0000e+00, 4.7670e-01, 1.1603e-01, 1.3101e-01, 4.1517e-03,\n",
       "        3.5169e-01, 3.9712e-03, 1.2216e-01, 0.0000e+00, 4.9712e-01, 4.2921e-03,\n",
       "        4.3523e-03, 4.4126e-01, 3.4235e-01, 0.0000e+00, 0.0000e+00, 2.4670e-02,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9343e-02,\n",
       "        4.9654e-01, 0.0000e+00, 4.7965e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0446e-02, 4.5199e-01,\n",
       "        0.0000e+00, 8.2232e-03, 0.0000e+00, 0.0000e+00, 4.2712e-01, 0.0000e+00,\n",
       "        4.7063e-01, 0.0000e+00, 0.0000e+00, 4.5157e-01, 0.0000e+00, 0.0000e+00,\n",
       "        1.9443e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        8.6043e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.1718e-03, 0.0000e+00,\n",
       "        4.8635e-01, 0.0000e+00, 0.0000e+00, 4.7809e-01, 5.0240e-01, 8.7246e-03,\n",
       "        0.0000e+00, 5.0516e-01, 0.0000e+00, 0.0000e+00, 3.9712e-03, 4.3814e-01,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 7.7238e-02, 8.0226e-03, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 2.9042e-02, 0.0000e+00, 7.4009e-03, 8.2633e-03,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 6.9757e-02, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        4.2319e-03, 0.0000e+00, 5.0314e-01, 4.5930e-03, 0.0000e+00, 0.0000e+00,\n",
       "        1.1553e-02, 3.5115e-01, 0.0000e+00, 2.6731e-01, 4.9205e-01, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 7.6616e-03, 5.1136e-01, 4.9931e-01,\n",
       "        2.5394e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3426e-01, 4.7935e-03,\n",
       "        5.0717e-01, 2.4068e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0718e-02,\n",
       "        0.0000e+00, 0.0000e+00, 4.6331e-03, 4.1517e-03, 0.0000e+00, 0.0000e+00,\n",
       "        3.2032e-01, 4.0715e-03, 0.0000e+00, 0.0000e+00, 8.5240e-03, 2.4704e-01,\n",
       "        8.4839e-03, 0.0000e+00, 4.3322e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        1.9696e-02, 0.0000e+00, 0.0000e+00, 2.0057e-05, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 2.0498e-02, 3.4698e-03, 3.6258e-01, 0.0000e+00,\n",
       "        4.1918e-03, 3.9913e-03, 0.0000e+00, 2.4664e-01, 0.0000e+00, 4.5440e-01,\n",
       "        8.8249e-03, 0.0000e+00, 3.2042e-01, 0.0000e+00, 1.2515e-02, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 5.1892e-01, 0.0000e+00, 4.8386e-01, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 2.8641e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 5.1525e-02, 8.5240e-03, 8.9653e-03, 0.0000e+00, 4.8749e-01,\n",
       "        0.0000e+00, 0.0000e+00, 8.5040e-03, 2.4373e-01, 4.7889e-01, 0.0000e+00,\n",
       "        1.9714e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4898e-01, 5.1096e-01,\n",
       "        0.0000e+00, 2.4670e-02, 5.8525e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 8.5642e-03, 0.0000e+00, 3.9511e-03, 0.0000e+00, 3.8709e-03,\n",
       "        2.1360e-02, 0.0000e+00, 3.5183e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        4.0420e-01, 4.1918e-03, 3.9110e-03, 5.0097e-01, 0.0000e+00, 4.8407e-01,\n",
       "        0.0000e+00, 7.8622e-03, 0.0000e+00, 0.0000e+00, 1.2174e-02, 3.8107e-03,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 4.8998e-01, 0.0000e+00, 0.0000e+00,\n",
       "        3.7105e-03, 1.2275e-02, 0.0000e+00, 0.0000e+00, 2.0913e-01, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 4.7522e-01, 3.3896e-03, 0.0000e+00, 4.7356e-01,\n",
       "        4.0436e-01, 4.9227e-01, 2.0518e-02, 0.0000e+00, 0.0000e+00, 4.2921e-03,\n",
       "        0.0000e+00, 0.0000e+00, 1.5933e-01, 0.0000e+00, 4.7239e-01, 0.0000e+00,\n",
       "        1.3879e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4790e-01,\n",
       "        2.0343e-01, 4.3723e-03, 0.0000e+00, 0.0000e+00, 7.7659e-02, 0.0000e+00,\n",
       "        0.0000e+00, 4.4526e-02, 8.1831e-03, 4.9674e-01, 0.0000e+00, 4.8747e-01,\n",
       "        4.1024e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.7672e-01, 0.0000e+00,\n",
       "        3.8910e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5441e-03, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 1.2295e-02, 4.6040e-01, 5.3118e-01, 4.2520e-03,\n",
       "        0.0000e+00, 1.0660e-01, 4.3587e-01, 1.2475e-02, 1.6527e-02, 1.7028e-02,\n",
       "        0.0000e+00, 0.0000e+00, 3.3876e-02, 0.0000e+00, 4.7813e-01, 2.0057e-05,\n",
       "        2.0057e-05, 0.0000e+00, 0.0000e+00, 4.2520e-03, 5.2023e-01, 4.1116e-03,\n",
       "        3.5829e-01, 0.0000e+00, 4.0915e-03, 0.0000e+00, 1.2134e-02, 4.8822e-01,\n",
       "        4.6499e-01, 0.0000e+00, 4.1214e-01, 0.0000e+00, 4.3322e-03, 5.1204e-01,\n",
       "        7.8020e-03, 0.0000e+00])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_embeddings = (sparse_embeddings != 0).float()\n",
    "total_activations = binary_embeddings.sum(dim=0)\n",
    "activation_densities = total_activations / binary_embeddings.shape[0]\n",
    "activation_densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_densities[activation_densities == 0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nouns-mech-interp-YeAXWnnL-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
